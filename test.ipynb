{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")  # Use a summarization model\n",
    "\n",
    "# Create a summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(input_string):\n",
    "    \"\"\"\n",
    "    Summarizes the input string using the specified model.\n",
    "    \n",
    "    Args:\n",
    "        input_string (str): The text to be summarized.\n",
    "        \n",
    "    Returns:\n",
    "        str: The summary of the input string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Summarize the input string\n",
    "        summary = summarizer(input_string, max_length=130, min_length=30, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_string = \"\"\"UNIVERSITY OF DELHI\n",
    "THE UNDERMENTIONED CANDIDATES OF B.E. EIGHTH SEMESTER EXAMINATION HELD IN\n",
    "MAY/JUNE, 2017 HAVING PASSED THE REMAINING SEMESTER EXAMINATION HELD IN NOV,\n",
    "2017, THUS HAVING PASSED ALL THE SEMESTERS EXAMINATIONS, THE UNDER\n",
    "MENTIONED REREGISTERED CANDIDATES IS DECLARED TO HAVE PASSED THE\n",
    "EXAMINATION FOR THE DEGREE OF BACHELOR OF ENGINEERING (B.E.):-\n",
    "NETAJI SUBHAS INSTITUTE OF TECHNOLOGY\n",
    "B.E. (MANUFACTURING PROCESSES AND AUTOMATION ENGINEERING)\n",
    "ROLLNO NAME AVERAGE CREDITS CLASS\n",
    "EARNED AWARDED\n",
    "668MP13 VANSH MENDIRATTA 59.85 228 SECOND CLASS    \n",
    "AK\n",
    "AN)\n",
    "CONTROELER OF EXAMINATIONS\n",
    "tp\n",
    " \n",
    "- or\n",
    "FACULTY OF TECHNOLOGY\n",
    "patep:- | 5 AUG 2024\n",
    "Owe FA.\n",
    "Scanned with CamScanner\n",
    "s.NO/NOT/2024/.4! 7 sade\n",
    "UNIVERSITY OF DELHI\n",
    "THE UNDERMENTIONED CANDIDATES OF B.E. EIGHTH SEMESTER EXAMINATION HELD IN\n",
    "MAY/JUNE, 2005 HAVING PASSED THE REMAINING SEMESTER EXAMINATION HELD IN\n",
    "MARCH, 2023, THUS HAVING PASSED ALL THE SEMESTERS EXAMINATIONS, THE UNDER\n",
    "MENTIONED REREGISTERED CANDIDATES IS DECLARED TO HAVE PASSED THE\n",
    "EXAMINATION FOR THE DEGREE OF BACHELOR OF ENGINEERING (B.E.):-\n",
    "NETAJI SUBHAS INSTITUTE OF TECHNOLOGY / PRIVATE    \n",
    "B.E. INSTRUMENTATION AND CONTROL ENGINEERING)      \n",
    "ROLLNO NAME AVERAGE CREDITS CLASS\n",
    "EARNED AWARDED\n",
    "460IC2K 1 PRASHANT BHORIA 59.85 228 SECOND CLASS   \n",
    "#RK\n",
    "A\n",
    "FACULTY\n",
    "\n",
    "OF TEYHNOLOGY CONTROLL F EXAMINATIONS\n",
    "patep: 9 5 AUG 2024\n",
    "dan\n",
    "Scanned with CamScanner\"\"\"\n",
    "print(summarize_text(input_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TALKKING TO MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained DialoGPT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat history\n",
    "chat_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(user_input):\n",
    "    \"\"\"\n",
    "    Generates a response to the user's input using DialoGPT.\n",
    "    \n",
    "    Args:\n",
    "        user_input (str): The user's input.\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    # Encode the user's input and append it to the chat history\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "    bot_input_ids = torch.cat([chat_history, new_user_input_ids], dim=-1) if chat_history else new_user_input_ids\n",
    "\n",
    "    # Generate a response from the model\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Extract and decode the model's response\n",
    "    chat_history_ids = chat_history_ids[:, bot_input_ids.shape[-1]:]\n",
    "    bot_response = tokenizer.decode(chat_history_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Update chat history\n",
    "    chat_history = chat_history_ids\n",
    "\n",
    "    return bot_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'chat_history' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: Goodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_with_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36mchat_with_model\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Encode the user's input and append it to the chat history\u001b[39;00m\n\u001b[0;32m     12\u001b[0m new_user_input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(user_input \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m bot_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([chat_history, new_user_input_ids], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchat_history\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m new_user_input_ids\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Generate a response from the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m chat_history_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(bot_input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'chat_history' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Example conversation\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_model(user_input)\n",
    "    print(f\"Bot: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
